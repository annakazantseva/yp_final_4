# -*- coding: utf-8 -*-
"""kazantseva-final-project-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1erNM1fVh93lp_cLtoYauhPHgKshIOb6y

## Описание проекта
Пользователи фотохостинга для профессиональных фотографов «Со Смыслом» (“With Sense”)размещают свои фотографии на хостинге и сопровождают их полным описанием: указывают место съёмок, модель камеры и т. д. Отличительная особенность сервиса — описание: его может предоставить не только тот, кто размещает фотографию, но и другие пользователи портала.  
  
Необходимо разработать систему поиска референсных фотографий для фотографов. Суть поиска заключается в следующем: пользователь сервиса вводит описание нужной сцены. Например, такое: 'A man is crossing a mountain pass on a metal bridge.' Сервис выводит несколько фотографий с такой же или похожей сценой.  
  
Необходимо разработать демонстрационную версию поиска изображений по запросу (PoC ,Proof of Concept или Проверка концепции), чтобы продемонстрировать, что такой проект практически осуществим.

Для демонстрационной версии нужно выбрать лучшую  модель, которая получит векторное представление изображения, векторное представление текста, а на выходе выдаст число от 0 до 1 — и покажет, насколько текст и картинка подходят друг другу. На основе лучшей модели собрать предварительную версию продукта  
  

**Юридические ограничения:** В некоторых странах, где работает компания With Sense, действуют ограничения по обработке изображений: поисковым сервисам и сервисам, предоставляющим возможность поиска, запрещено без разрешения родителей или законных представителей предоставлять любую информацию, в том числе, но не исключительно, текстов, изображений, видео и аудио, содержащие описание, изображение или запись голоса детей. Ребёнком считается любой человек, не достигший 16-ти лет.
В вашем сервисе строго следуют законам стран, в которых работают. Поэтому при попытке посмотреть изображения, запрещённые законодательством, вместо картинок показывается дисклеймер: This image is unavailable in your country in compliance with local laws.  
  
Однако в PoC нет возможности воспользоваться данным функционалом. Поэтому необходимо очистить данные от проблемного контента. Во время тестирования модели при появлении в запросе “вредного” контента должен отображаться дисклеймер.

## Описание данных  
  
В файле train_dataset.csv находится информация, необходимая для обучения:
*   имя файла изображения,
*   идентификатор описания
*   текст описания.
  
Для одной картинки может быть доступно до 5 описаний.  
Идентификатор описания имеет формат <имя файла изображения>#<порядковый номер описания>.  
В папке train_images содержатся изображения для тренировки модели.
В файле CrowdAnnotations.tsv  — данные по соответствию изображения и описания, полученные с помощью краудсорсинга. Номера колонок и соответствующий тип данных:


*   Имя файла изображения.
*   Идентификатор описания.
*   Доля людей, подтвердивших, что описание соответствует изображению.
*   Количество человек, подтвердивших, что описание соответствует изображению.
*   Количество человек, подтвердивших, что описание не соответствует изображению.  

В файле ExpertAnnotations.tsv  — данные по соответствию изображения и описания, полученные в результате опроса экспертов. Номера колонок и соответствующий тип данных:
*   Имя файла изображения.
*   Идентификатор описания.
*   3, 4, 5 — оценки трёх экспертов.  
  
Эксперты ставят оценки по шкале от 1 до 4, где 1 — изображение и запрос совершенно не соответствуют друг другу, 2 — запрос содержит элементы описания изображения, но в целом запрос тексту не соответствует, 3 — запрос и текст соответствуют с точностью до некоторых деталей, 4 — запрос и текст соответствуют полностью.      
В файле test_queries.csv находится информация, необходимая для тестирования:
*    идентификатор запроса,
*    текст запроса
*    релевантное изображение.

Для одной картинки может быть доступно до 5 описаний. Идентификатор описания имеет формат <имя файла изображения>#<порядковый номер описания>.
В папке test_images содержатся изображения для тестирования модели.

Загрузим данные на Colab
"""

!wget -N https://code.s3.yandex.net/datasets/dsplus_integrated_project_4.zip

!unzip -uq dsplus_integrated_project_4.zip

!pip install transformers

"""Импортируем необходимые библиотеки"""

import os

import numpy as np
import pandas as pd
import math

import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.image as mpimg

import seaborn as sns
import re
import random
from PIL import Image

from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.linear_model import LinearRegression, Ridge

import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

import torch
import transformers

import tensorflow as tf
from tensorflow.keras.applications.resnet import (
    ResNet50,
    decode_predictions,
    preprocess_input,
)
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image

import joblib
from tqdm import tqdm, notebook
tqdm.pandas()

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
stop_words = stopwords.words('english')

RANDOM_SEED = 42
OUTPUT_DIR = '/content/working/'
TEST_IMG_DIR = '/content/to_upload/test_images/'

"""## Исследовательский анализ данных
Загрузим данные и проведем исследовательский анализ
"""

train_data = pd.read_csv('/content/to_upload/train_dataset.csv')
display(train_data.head())
train_data.info()

train_data.describe().T

crowd_annotations = pd.read_csv(
    '/content/to_upload/CrowdAnnotations.tsv',
    sep = '\t',
    header = None,
    names = ['image', 'query_id', 'pos_ratio', 'pos', 'neg']
)
display(crowd_annotations.head())
crowd_annotations.info()

crowd_annotations.describe(include = 'all').T

print(f"Полных дубликатов - {crowd_annotations.duplicated().sum()}, \
дубликатов пар фото:описание - {crowd_annotations.duplicated(subset = ['image', 'query_id']).sum()}")

expert_annotations = pd.read_csv(
    '/content/to_upload/ExpertAnnotations.tsv',
    sep = '\t',
    header = None,
    names = ['image', 'query_id', 'exp_1', 'exp_2', 'exp_3']
)
expert_annotations.info()

expert_annotations.describe(include = 'all').T

test_images = pd.read_csv('/content/to_upload/test_images.csv')
display(test_images.head())
test_images.info()

test_images.nunique()

test_queries = pd.read_csv('/content/to_upload/test_queries.csv', sep = '|', index_col = 0)
display(test_queries.head())
test_queries.info()

test_queries.describe(include = 'all').T

"""**Выводы:**
  
Были загружены данные: 1000 фотографий для обучения модели, 100 фотографий для тестирования модели, данные по соответствию изображения и описания, полученные с помощью краудсорсинга и в результате опроса экспертов;  
Данные были проверены на отсутствие пропусков и дубликатов;  
В условиях указано, что одному изображению могут соответствовать до 5-ти правильных описаний (запросов), однако, из сводки по train_dataset видно, что описаний больше, чем уникальных фотографий,  более чем в 5 раз.  
Уникальных описаний практически 1:1 с уникальными фото, это означает, что пары описание/фото перемешаны;
Возможно, описания одних фотографий могут также подходить и к другим и степень похожести могут определеить оценки полученные с помощью экспертов и краудсорсинга.

### Обучающий набор данных  
  
Проверим соответствие между названиями фотографий и идентификаторов описания, которые имеет формат <имя файла изображения>#<порядковый номер описания>.  
Выделим из идентификаторов название фотографии, к которой оно составлено и сопоставим с названием фотографии, к которой оно прикреплено:
"""

train_data['accordance'] = train_data.apply(lambda x: True if x['image'] == x['query_id'][:-2] else False, axis = 1)
train_data['accordance'].value_counts()

"""Таким образом, у 158 фотографий в обучающем наборе есть описание, которое составлено для них, этого количества фотографий не хватит для обучения модели  
Далее рассмотрим, насколько качественно размечают данные эксперты и "толпа" (краудсорсинг):

#### Экспертные оценки
"""

expert_annotations['accordance'] = expert_annotations.apply(lambda x: True if x['image'] == x['query_id'][:-2] else False, axis = 1)
display(expert_annotations.sample(5))
expert_annotations['accordance'].value_counts()

expert_annotations.loc[expert_annotations['accordance'] == True].describe()

expert_annotations.loc[expert_annotations['accordance'] == False].describe()

ax = sns.barplot(
    data = expert_annotations[['exp_1', 'exp_2', 'exp_3', 'accordance']].melt(
        id_vars = ['accordance'],
        value_vars = ['exp_1', 'exp_2', 'exp_3']
    ),
    y = 'value',
    x = 'accordance',
    hue = 'variable'
)
plt.xlabel('Соответствие описания и фото')
plt.ylabel('Оценка экспертов')
plt.title('Распределение оценок экспертов')
plt.show()

"""**Вывод:** фотографиям с подходящим описанием эксперты в более чем 75% случаев ставят максимальную оценку - 4;  
Фотографиям, которым сопоставлено описание от других фотографий эксперты ставят низшую оценку - 1 и только в 75% случаев - 2 и выше (иногда 4).

Чтобы объединить оценки экспертов и привести их к диапазону от 0 до 1, выполним обычное min/max шкалирование:
"""

expert_annotations['exp_ratio'] = expert_annotations.apply(lambda x: (x['exp_1'] + x['exp_2'] + x['exp_3'] - 3) / 9, axis = 1)
expert_annotations.head()

expert_annotations.describe().T

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))
sns.histplot(data = expert_annotations['exp_ratio'], bins = 50, ax = ax[0])
ax[0].set_xlabel('Значение комплексной оценки экспертов')
ax[0].set_ylabel('Количество оценок')
sns.barplot(data = expert_annotations, x = 'accordance', y = 'exp_ratio', ax = ax[1])
ax[1].set_xlabel('Соответствие описания и фото')
ax[1].set_ylabel('Значение комплексной оценки экспертов')
plt.suptitle('Распределение комплексной оценки экспертов', fontsize = 14)
plt.tight_layout()
plt.show()

"""**Вывод:** Комплексная оценка экспертов получилась дискретной, с шагом примерно 0.1 и отражает выводы, которые были сделаны ранее.

### Оценки толпы

Исследуем оценоки краудсорсинга и дополнительно посчитаем, сколько человек участвуют в опросе на краудсорсинговой платформе (это понадобится далее для расчета комплексной оценки):
"""

crowd_annotations['accordance'] = crowd_annotations.apply(lambda x: True if x['image'] == x['query_id'][:-2] else False, axis = 1)
crowd_annotations['crowd_cnt'] = crowd_annotations['pos'] + crowd_annotations['neg']
crowd_annotations.describe().T

crowd_annotations['accordance'].value_counts()

fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))
sns.histplot(data = crowd_annotations['pos_ratio'], bins = 50, ax = ax[0])
ax[0].set_xlabel('Значение комплексной оценки на краудсорсинге')
ax[0].set_ylabel('Количество оценок')
sns.barplot(data = crowd_annotations, x = 'accordance', y = 'pos_ratio', ax = ax[1])
ax[1].set_xlabel('Соответствие описания и фото')
ax[1].set_ylabel('Значение комплексной оценки на краудсорсинге')
plt.suptitle('Распределение комплексной оценки экспертов', fontsize = 14)
plt.tight_layout()
plt.show()

"""**Вывод:** В опросах участвуют от 3 до 6 человек. "Толпа" хорошо размечает данные, как и эксперты: оценки фотографиям с подходящими оценками значительно выше, чем тем, для которых описание уот других фото.  
 В наборе с краудсорсинговыми моделями больше фотографий, к которым приведено собственное описание, при составлении обучающего набора это увеличит его.

### Расчет комплексной оценки

Посмотрим, сколько наивысших оценок в таблице с данными краудсориснга и аналогично для экспертых:
"""

crowd_annotations.loc[crowd_annotations['pos_ratio'] == 1, 'pos_ratio'].count()

expert_annotations.loc[expert_annotations['exp_ratio'] == 1, 'exp_ratio'].count()

"""1570 - это больше, чем фото с собственным описанием, значит, часто получается так, что описание было составлено не для рассматриваемой фотографии, но также подходит к нему (возможно, описание носит обощенный характер). Объединим все таблицы, за основу возьмем таблицу с оценками толпы, так как она больше:"""

train_result_df = crowd_annotations.merge(expert_annotations[['image', 'query_id', 'exp_ratio']], how = 'outer', on = ['image', 'query_id'])
train_result_df = train_result_df.rename(columns = {'pos_ratio' : 'crowd_ratio'})
display(train_result_df.head())
print(train_result_df.shape)

train_result_df.info()

train_result_df = (train_result_df
                   .merge(train_data[['query_id', 'query_text']].drop_duplicates(),
                                           how = 'inner',
                                           on = 'query_id')
                   .drop(columns = ['pos', 'neg', 'accordance', 'query_id'])
                  )
train_result_df = train_result_df[['image', 'query_text', 'crowd_ratio', 'crowd_cnt', 'exp_ratio']]
display(train_result_df.head())
print(train_result_df.shape)

print(train_result_df.info())

"""Пропущенные значения в оценках заполним нулями (их нет), а также создадим колонку с количеством экспертов, мы знаем, что их всегда 3, но для расчета комплексной оценки - сделаем:"""

train_result_df['exp_cnt'] = train_result_df['exp_ratio'].notna().astype('int')*3
train_result_df.fillna(0, inplace = True)

"""Рассчитаем сводную оценку экспертов и толпы, несмотря на то, что краудсорсеры также хорошо оценивают описания, как и эксперты, думаю будет опрадано увеличить вес экспертных оценок до 2."""

EXP_WEIGHT = 2
def complex_estimation(row):
    return (EXP_WEIGHT*row['exp_ratio']*row['exp_cnt'] + row['crowd_ratio']*row['crowd_cnt']) / (EXP_WEIGHT*row['exp_cnt'] + row['crowd_cnt'])
train_result_df['complex_est'] = train_result_df.apply(complex_estimation , axis = 1)
display(train_result_df.head())

train_result_df.drop(columns = ['crowd_ratio', 'crowd_cnt', 'exp_ratio', 'exp_cnt'], inplace = True)
print(train_result_df.info())

sns.histplot(data = train_result_df['complex_est'], bins = 50)
plt.xlabel('Сводная оценка')
plt.ylabel('Количество оценок')
plt.title('Распределение сводных оценок')
plt.show()

"""**Вывод:** Распределение комплексной оценки похоже на распределение оценок "толпы", т.к. данных по их оценкам больше.
Значения сводной оценки в диапазоне от 0 до 1 и при этом не занижены, если эксперты или "толпа" не оценивали пару (как в случае с обычным усреднением или иным взятием в пропорции).

### Выводы:
  
158 фотографий в тренировочном наборе имеют описание, которое составлено непосредственно для них;
Эксперты и "толпа" (краудсорсинг), в целом, качественно размечают данные;  
Фотографиям с подходящим описанием эксперты в более чем 75% случаев ставят высшую оценку - 4, а тем, которым сопоставлено описание от других фотографий эксперты чаще ставят низшую оценку - 1 и только в 75% случаев - 2 и выше (иногда 4);  
В опросах краудсорсинга участвуют от 3 до 6 человек, это увеличивает шансы на объективность оценки;  
Оценки экспертов были приведены к единой оценке с диапазоном от 0 до 1 и далее объединены с оценками "толпы" в комплексную оценку.

## Исключение запрещенного контента

Очистим обучающие данные от проблемного контента, фотографий детей.  
Создадим функцию child_protect, которая будет лемматизировать запрос и искать слова из специально составленного списка stop_words прямо или косвенно связанные с детьми и возвращать True в случае их обнаружения, а в противном случае False.
При попытке посмотреть изображения, запрещённые законодательством, вместо картинок должен показывается дисклеймер: This image is unavailable in your country in compliance with local laws.
"""

words = re.compile(r'[^a-zA-Z]+')
lemmatizer = WordNetLemmatizer()

def nlp_preprocessing(text):

    global lemmatizer

    text = text.lower()
    text = re.sub(r"´", "'", text) # иногда вместо одинарной кавычки ' используется ´
    text = re.sub(words, ' ', text) # оставляем только слова
    text = re.sub('\s+', ' ', text) # удалим лишние пробелы между словами
    text = text.strip(' ') # удалим лишние пробелы в конце и начале строки

    tagged_words = nltk.pos_tag(word_tokenize(text)) # разделим текст на отдельные слова и их POS-теги
    wn_tags = {'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV, 'J': wordnet.ADJ} # для преобразования POS-тегов в формат WordNet

    # приведем формы слов к начальным (лемматизируем) с учетом POS-тега
    lemmatized_words = []
    for word, tag in tagged_words:
        if tag[0] in wn_tags:
            lemma = lemmatizer.lemmatize(word, wn_tags[tag[0]])
        else:
            lemma = lemmatizer.lemmatize(word)
        lemmatized_words.append(lemma)

    text = ' '.join(lemmatized_words)
    return text

stop_words = [
    "boy",
    "girl",
    "child",
    "baby",
    "kid",
    "babe",
    "infant",
    "teenager",
    "teeny",
    "juvenile",
    "teener",
    "stripling",
    "young",
    "toddler"]

childs = re.compile(r'|'.join(stop_words))

def child_protect(text):
    text_lemm = nlp_preprocessing(text)
    return False if re.search(childs, text_lemm) == None else True

nltk.download('averaged_perceptron_tagger')

train_result_df['query_prohibited'] = train_result_df['query_text'].apply(child_protect)
display(train_result_df[['query_text', 'query_prohibited']].sample(n = 10, random_state = 12345))

"""Очистим обучающий набор данных от фото с детьми, для этого найдем все пары фото:описание, где в описании упоминается ребенок и при этом косплексная оценка соответствия описания к фото больше 0.5, что можно вербально интерпретировать как "скорее да, чем нет"."""

child_images = set(train_result_df.loc[(train_result_df['query_prohibited'] == True) & (train_result_df['complex_est'] > 0.5), 'image'])
print(len(child_images))

train_result_df['image_prohibited'] = train_result_df['image'].isin(child_images)
train_result_df.head()

TRAIN_IMG_DIR = '/content/to_upload/train_images/'

"""Проверим работу функции:"""

rows = 2
columns = 3

fig = plt.figure(figsize=(18, 10))
count = 1
for idx, file, query_text in train_result_df[['image', 'query_text']].sample(rows * columns, random_state = RANDOM_SEED).itertuples():
    ax = fig.add_subplot(rows, columns, count)
    count += 1

    img = mpimg.imread(TRAIN_IMG_DIR + file)
    ax.imshow(np.array(img))

    height, width, channels = img.shape
    chunks, chunk_size = query_text.split(), 10

    # Создание прямоугольника-рамки для всего графика
    if train_result_df.iloc[idx].query_prohibited or train_result_df.iloc[idx].image_prohibited:
        if train_result_df.iloc[idx].image_prohibited:
            rect = patches.Rectangle((2, 1), width-4, height-3, linewidth = 3, edgecolor='r', facecolor='none', ls = '--')
        else:
            rect = patches.Rectangle((2, 1), width-4, height-3, linewidth = 3, edgecolor='lime', facecolor='none', ls = '--')
        ax.add_patch(rect)
        if train_result_df.iloc[idx].query_prohibited:
            plt.title('\n'.join([' '.join(chunks[i : i + chunk_size]) for i in range(0, len(chunks), chunk_size)]),
                      fontdict = {'fontsize': 14, 'color' : 'r'})
        else:
            plt.title('\n'.join([' '.join(chunks[i : i + chunk_size]) for i in range(0, len(chunks), chunk_size)]),
                      fontdict = {'fontsize': 14, 'color' : 'lime'})
    else:
        plt.title('\n'.join([' '.join(chunks[i : i + chunk_size]) for i in range(0, len(chunks), chunk_size)]),
                  fontdict = {'fontsize': 14, 'color' : 'black'})


    plt.axis('off')

plt.suptitle('Примеры текстовых сопровождений и фото, содержание которых запрещено законодательством', fontsize = 16)
plt.tight_layout()
plt.show()

"""Фото и описание черного цвета говорят о том, что они никак не связаны с детьми. Если описание красное, то такой запрос будет отклоняться системой, а если фото обведено красным, то на нем изображен ребенок. При этом важно обратить внимание: в левом верхнем углу, изображен ребенок, но описание не содержит его упоминания. Такая пара будет также удалена из обучающего набора:"""

idx_prohibited = train_result_df.loc[(train_result_df['query_prohibited'] == True) | (train_result_df['image_prohibited'] == True)].index
print(f'Всего объектов, которые необходимо удалить из набора: {len(idx_prohibited)}')

train_result_df = train_result_df.drop(index = idx_prohibited).reset_index(drop = True).drop(columns = ['query_prohibited', 'image_prohibited'])
print(f'Размеры датасета после предобработки: {train_result_df.shape}')

# выведем несколько строк полученного датасета:
train_result_df.head()

"""### Векторизация исходных данных
Векторизация текста с помощью BERT

Для векторизации текстового описания будем использовать базовую версию BERT.

План работы:

    Загрузка предобученной модели BERT и токенизатора;
    Токенизация текста в номера токенов из словаря методом encode();
    Приведение векторов к одному размеру путем прибавления к более коротким векторам идентификатора 0 (padding);
    Создание маски внимания (attention_mask), которая указывает, что прибавленные 0 не несут информации;
    Подготовка эмбеддингов.

Для моделей BERT не требуется предварительная обработка текста, т.к. там есть свой токенизатор, который сразу переводит текст в понятные модели теги, поэтому будем подавать в BERT "сырой" текст:

"""

model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, 'bert-base-uncased')

tokenizer_BERT = tokenizer_class.from_pretrained(pretrained_weights)
model_BERT = model_class.from_pretrained(pretrained_weights)

# переместим модель на GPU:
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_BERT = model_BERT.to(device)

# выделим только уникальные запросы:
doc = train_result_df['query_text'].drop_duplicates().reset_index(drop = True).astype('string')
display(doc.head())
print(f'\nКоличество уникальных запросов: {doc.shape[0]}')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# file_name = OUTPUT_DIR + 'text_embeddings.pkl'
# if not os.path.isfile(file_name) or RECNT_NBOOK:
#     # загрузим предобученную модель и токенизатор DistilBERT:
#     #model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')
# 
#     # если необходимо использовать BERT вместо DistilBERT, нужно раскомментировать следующую строку:
#     model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, 'bert-base-uncased')
# 
#     tokenizer_BERT = tokenizer_class.from_pretrained(pretrained_weights)
#     model_BERT = model_class.from_pretrained(pretrained_weights)
# 
#     # переместим модель на GPU:
#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#     model_BERT = model_BERT.to(device)
#     print('Устройство:', device)
# 
#     # токенизируем тексты:
#     # укажем аргумент add_special_tokens = True, чтобы к тексту добавлялся токен начала (101) и токен конца текста (102)
#     # укажем аргумент truncation = True, чтобы тексты длиннее 512 токенов обрезались (ограничение модели BERT)
#     tokenized = doc.progress_apply(lambda x: tokenizer_BERT.encode(x, truncation=True, add_special_tokens=True))
# 
#     # найдём максимальную длину векторов после токенизации:
#     max_len = max(len(sublist) for sublist in tokenized.values)
#     print(f'Максимальная длина векторов после токенизации: {max_len}')
# 
#     # приведем вектора к одному размеру:
#     padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])
# 
#     # создадим маску для важных токенов:
#     attention_mask = np.where(padded != 0, 1, 0)
# 
#     # эмбеддинги будем создавать батчами, чтобы хватило памяти:
#     batch_size = 100
#     embeddings = []
#     for i in notebook.tqdm(range(padded.shape[0] // batch_size + 1)):
#         lower_edge = batch_size*i
#         upper_edge = batch_size*(i+1) if batch_size*(i+1) < padded.shape[0] else padded.shape[0]
#         # преобразуем данные в формат тензоров:
#         batch = torch.LongTensor(padded[lower_edge:upper_edge])
#         attention_mask_batch = torch.LongTensor(attention_mask[lower_edge:upper_edge])
# 
#         # переместим входные данные на GPU:
#         batch = batch.to(device)
#         attention_mask_batch = attention_mask_batch.to(device)
# 
#         # для ускорения вычисления функцией no_grad() в библиотеке torch укажем, что градиенты не нужны, модель BERT обучать не будем:
#         with torch.no_grad():
#             batch_embeddings = model_BERT(batch, attention_mask = attention_mask_batch)
# 
#         # из полученного тензора извлечём нужные элементы и добавим в список всех эмбеддингов,
#         # перенесем данные с GPU методом cpu() и преобразуем элементы методом numpy() к типу numpy.array:
#         embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())
#         joblib.dump(embeddings, file_name)
# else:
#     embeddings = joblib.load(file_name)
# embeddings = np.concatenate(embeddings)
# embeddings.shape

doc = pd.DataFrame(doc)
doc['query_vector'] = embeddings.tolist()
doc['query_vector'] = doc['query_vector'].apply(lambda x: tf.convert_to_tensor([x], dtype=np.float64))
display(doc.head())
print(f'\nРазмер вектора описания картинки: {doc.loc[0, "query_vector"].shape[1]}')

"""### Векторизация изображений с помощью ResNet50

Для векторизации изображение будем использовать нейронную сеть ResNet50, предобученную на imagenet. Мы не будем ее переобучать и не будем задавать выходной слой, чтобы забрать только эмбеддинги:
"""

def create_model(input_shape):
    optimizer = Adam(learning_rate=0.0001)
    backbone = ResNet50(input_shape=input_shape, include_top = False, weights='imagenet')
    backbone.trainable = False
    model = Sequential()
    model.add(backbone)
    model.add(GlobalAveragePooling2D(trainable = False))
    model.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mae'])

    return model

def image_to_array(file: str, path: str = TRAIN_IMG_DIR, wh: int = 224, ht: int = 224):
    img = image.load_img(path + file, target_size=(wh, ht))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    return x

model = create_model((224, 224, 3))

# выделим только уникальные изображения:
image_vec = pd.DataFrame(train_result_df['image'].drop_duplicates().reset_index(drop = True))
display(image_vec.head())
print(f'\nКоличество уникальных изображений: {image_vec.shape[0]}')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# file_name = OUTPUT_DIR + 'img_embeddings.pkl'
# if not os.path.isfile(file_name) or RECNT_NBOOK:
#     img_embeddings = image_vec.apply(lambda x: tf.cast(model(image_to_array(x['image'])), dtype=np.float64), axis=1)
#     joblib.dump(img_embeddings, file_name)
# else:
#     img_embeddings = joblib.load(file_name)
# image_vec['image_vector'] = img_embeddings

display(image_vec.head())
print(f'\nРазмер вектора изображения: {image_vec.loc[0, "image_vector"].shape[1]}')

"""**Выводы**

Для работы будущей модели поиска изображений по текстовому запросу исходные данные были векторизованы следущим способом:

    текстовые запросы из тренировочного набора с помощью стандартной модели BERT, размер эмбеддинга на выходе - 768;
    изображения из тренировочного набора с помощью сверточной нейронной сети ResNet50, размер эмбеддинга на выходе - 2048.

### Подготовка данных для обучения

В данном разделе отберем данные для обучения и разделим их на тренировочную и валидационную выборки. Примем следующую логику работы для будущей модели поиска изображений по текстовому запросу:

    на вход модель получает векторизованный эмбеддинг текстового запроса с размерностью 768;
    переводит его в некоторое приближенное подобие векторизованного эмбеддинга изображения, которое такому запросу должно соответствовать, с размерностью 2048.

Чтобы научить модель выполнять такие вычисления (преобразования), мы должны отобрать пары описание-изображение, в которых изображение полностью или частично подходит описанию. Сделаем это, опираясь на комплексную оценку - если оценка выше 0.5, значит описание больше подходит, чем нет.

Примечание: отбор по комплексной оценке нельзя считать некоторым упрощением задачи, т.к. для проверки работы модели мы будем искать соответствие одному текстовому запросу по всему набору, в котором большинство картинок ему не соответствует в силу объективных причин.

Объединим векторизованные эмбеддинги с общим набором данных:
"""

train_vectorized_df = train_result_df.merge(doc, how = 'left', on = 'query_text')
train_vectorized_df = train_vectorized_df.merge(image_vec, how = 'left', on = 'image')
display(train_vectorized_df.head())
print(train_vectorized_df.info())

accordance_idx = train_vectorized_df.loc[train_vectorized_df['complex_est'] > 0.5, 'image'].index
print(f'Итого изображений, степень совместимости с описанием у которых > 50%: {len(accordance_idx)}')

"""### Tренировочная и тестовая выборки для обучения модели
Для обучения разделим датасет на тренировочную и тестовую выборки. Простое случайное разбиение не подходит: нужно исключить попадание изображения и в обучающую, и в тестовую выборки. Для этого воспользуемся классом GroupShuffleSplit из библиотеки sklearn.model_selection:
"""

gss = GroupShuffleSplit(n_splits = 1, train_size = 0.75, random_state = RANDOM_SEED)
train_idx, valid_idx = next(
    gss.split(
        X = train_vectorized_df.loc[accordance_idx].drop(columns=['complex_est']),
        y = train_vectorized_df.loc[accordance_idx, 'complex_est'],
        groups = train_vectorized_df.loc[accordance_idx, 'image']))
train_df, valid_df = train_vectorized_df.loc[train_idx], train_vectorized_df.loc[valid_idx]
train_df.reset_index(drop = True, inplace = True)
valid_df.reset_index(drop = True, inplace = True)

# оставим для обучения только текст запроса, название изображений и их векторизованные сущности:
X_train = train_df[['query_text', 'query_vector']]
y_train = train_df[['image', 'image_vector']]

X_valid = valid_df[['query_text', 'query_vector']]
y_valid = valid_df[['image', 'image_vector']]

X = tf.stack(X_train['query_vector'])
Y = tf.stack(y_train['image_vector'])

X_val = tf.stack(X_valid['query_vector'])
Y_val = tf.stack(y_valid['image_vector'])

text_vector_size = X.shape[2]
image_vector_size = Y.shape[2]

print(f'Тренировочный набор данных: {X.shape, Y.shape}')
print(f'Валидационный набор данных: {X_val.shape, Y_val.shape}')

"""Для будущей работы модели и ее проверки напишем несколько функций:

   * get_text_embedding - служит для получения эмбеддинга с помощью BERT "на лету", так можно проверять поиск по любому запросу, даже тому, которого не было в исходных данных (единственное условие - запрос должен быть на английском языке);
   * cosine_distance - вычисление косинусного рассотяния и его приведения к диапазону от 0 до 1, именно эту метрику будем использовать для опредления степени уверенности модели в выборе изображений;
   * show_images - выводим найденные изображения;
   * search_image - основная функция, которая объединяет в себе весь процесс поиска и верификации изображений: получает векторное представление текста, векторные представления изображений из базы, рассчитывает для каждой пары метрику на основе косинусного расстояния от 0 до 1, которая показывает, насколько текст и картинка подходят друг другу и отбирает lim лучших изображений, подходящих по строке запроса; для работы в нее необходимо передать предобученную модель.


"""

def get_text_embedding(query_text):
    """Получение эмбеддинга текста с помощью BERT на лету из строки
    """
    global tokenizer_BERT, model_BERT

    tokenized = tokenizer_BERT.encode(query_text, truncation=True, add_special_tokens=True)
    tokenized = np.array([i for i in tokenized])
    attention_mask = np.where(tokenized != 0, 1, 0)

    tensor = torch.LongTensor(tokenized)
    attention_mask = torch.LongTensor(attention_mask)

    # переместим входные данные на GPU:
    tensor = tensor.to(device)
    attention_mask = attention_mask.to(device)

    with torch.no_grad():
        embedding = model_BERT(tensor.unsqueeze(0), attention_mask = attention_mask.unsqueeze(0))

    embedding = embedding[0][:,0,:].cpu().numpy()
    embedding = tf.convert_to_tensor(embedding, dtype = np.double)

    return embedding

def cosine_distance(A: tf.Tensor, B: tf.Tensor):
    """Вычисление косинусного рассотяния и его приведение к диапазону от 0 до 1
    """
    A_float = tf.cast(A, dtype=np.float32)
    B_float = tf.cast(B, dtype=np.float32)
    cosine_similarity = (tf.reduce_sum(tf.multiply(A_float, B_float), axis = -1) / (tf.norm(A_float, axis = -1) * tf.norm(B_float, axis = -1))).numpy()
    cosine_distance = 1 - cosine_similarity # результат в диапазоне [0; 2], где 0 - максимальное сходство, а 2 - максимальное расстояние
    result = 1 - (cosine_distance / 2) # приводим к диапазону от 0 до 1 (в т.ч. "переворачиваем" его)

    return result

def show_images(query, images, *, path = TRAIN_IMG_DIR, suptitle = ''):
    columns = 4
    rows = math.ceil(len(images) / columns)
    fig = plt.figure(figsize=(4*columns, 4*rows))
    plt.suptitle(suptitle + query)
    count = 1
    for file, dist in images:
        img = Image.open(path + file)
        fig.add_subplot(rows, columns, count)
        count += 1
        plt.title(round(dist, 3))
        plt.imshow(np.array(img))
        plt.axis('off')
    plt.tight_layout()
    #plt.show()
    return None

def search_image(query_text, images_base, lim, predict_func, *args, **kwargs):
    """Основная функция, которая объединяет в себе весь процесс поиска и верификации изображений ищет изображение по строке запроса,
    для работы в нее необходимо передать предобученную модель и аргументы для нее, в случае необходимости
    """
    # images_base с полями image, image_vector и (complex_est)
    query_lemm = nlp_preprocessing(query_text)
    # проверим нет ли юридических ограничений для запрашиваемого контента:
    if child_protect(query_lemm):
        print ('This image is unavailable in your country in compliance with local laws')
        return None

    # обработаем текст с помощью BERT:
    query_embedding = get_text_embedding(query_text)

    # получим представление вектора текста в виде вектора изображения, с помощью предсказания модели:
    query_image = predict_func(query_embedding, *args, **kwargs)

    # рассчитаем расстояния от полученного вектора до всех изображений в базе images_base
    distances = pd.Series(index = images_base.index, dtype = 'float64')
    for idx in images_base.drop_duplicates(subset = 'image').index:
        distances.loc[idx] = cosine_distance(query_image, images_base.loc[idx, 'image_vector'])

    best_match = distances.dropna().sort_values(ascending = False).iloc[:lim].index

    return [(images_base.loc[idx, 'image'], distances.loc[idx]) for idx in best_match]



"""## Обучение моделей для сопоставления текста и изображения

Обучим две модели машинного обучения и нейронную сеть, которые должны будут сопоставлять текстовое описание и изображение, которое ему подходит. Для контроля обучения и сравнения результатов будем пользоваться метрикой MSE, т.к. зачастую нейронные сети с функцией потерь MSE обучаются быстрее:

### Линейная регрессия
"""

def fit_LR(X_tensor, y_tensor, model = LinearRegression()):
    return model.fit(tf.reshape(X_tensor, shape=(-1, X_tensor.shape[-1])), tf.reshape(y_tensor, shape=(-1, y_tensor.shape[-1])))

def predict_LR(X_tensor, model):
    y_pred = model.predict(tf.reshape(X_tensor, shape=(-1, X_tensor.shape[-1])))
    return tf.convert_to_tensor(y_pred)

def mse_LR(y_pred, y_true):
    return mean_squared_error(tf.reshape(y_pred, shape=(-1, y_pred.shape[-1])), tf.reshape(y_true, shape=(-1, y_true.shape[-1])))

lr = fit_LR(X, Y, LinearRegression())
y_pred = predict_LR(X_val, lr)
print(f'MSE модели линейная регрессия на валидационной выборке: {mse_LR(y_pred, Y_val)}')

"""Проверим работу модели на двух примерах из валидационного набора.

Примечание: в скобках над каждым изображением указана цифра от 0 до 1, которая показывает вероятность соответствия изображения тексту.
"""

query_text = 'A person is standing on skis in the snow with a dog'
print(f'Looking for pictures under the description: {query_text}')
images = search_image(query_text, y_valid, 4, predict_LR, lr)
if images:
    show_images(query_text, images)

"""Людей модель нашла, лыжи и собак нет.  
  
Ниже результат работы модели со случайным поиском по валидационной выборке для экспериментов:

"""

query_text = random.choice(X_valid['query_text'])
print(f'Looking for pictures under the description: {query_text}')
images = search_image(query_text, y_valid, 4, predict_LR, lr)
if images:
    show_images(query_text, images)



"""### Ridge Regressor"""

ridge = fit_LR(X, Y, Ridge())
y_pred = predict_LR(X_val, ridge)
print(f'MSE модели Ridge на валидационной выборке: {mse_LR(y_pred, Y_val)}')

"""Метрика немного лучше, чем у предыдущей модели"""

query_text = 'A person is standing on skis in the snow with a dog'
print(f'Looking for pictures under the description: {query_text}')
images = search_image(query_text, y_valid, 4, predict_LR, ridge)
if images:
    show_images(query_text, images)

"""Однако, лыжи она смогла найти. Собак нет

Результат случайного поиска
"""

query_text = random.choice(X_valid['query_text'])
print(f'Looking for pictures under the description: {query_text}')
images = search_image(query_text, y_valid, 4, predict_LR, ridge)
if images:
    show_images(query_text, images)



"""### Нейронная сеть

Посмотрим, что находится внутри подготовленных эмбеддиногов:
"""

tensor_np = X.numpy()
min_value = np.min(tensor_np)
max_value = np.max(tensor_np)
print(f'Text vector range is ({min_value})-({max_value})')

tensor_np = Y.numpy()
min_value = np.min(tensor_np)
max_value = np.max(tensor_np)
print(f'Image vector range is ({min_value})-({max_value})')

"""На вход модели могут поступать векторы с отрицательными значениями , значит первые слои должны быть способны их обрабатывать. Примем тангенциальную для первого слоя и leaky_relu для второго, чтобы мягко перевести значения меньше нуля в положительные.

Кроме того, для контроля переобучения добавим слои Dropout, а для воспроизводимости кода зафиксируем состояние random_state и инициализируем веса в слоях с помощью GlorotNormal:

"""

def create_nn_model():
    tf.random.set_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    random.seed(RANDOM_SEED)

    optimizer = Adam()
    model = Sequential()
    model.add(Dense(text_vector_size,
                    activation='tanh',
                    kernel_initializer = tf.keras.initializers.GlorotNormal(seed = RANDOM_SEED)
                   ))
    model.add(Dropout(0.5))
    model.add(Dense(text_vector_size // 2,
                    activation='leaky_relu',
                    kernel_initializer = tf.keras.initializers.GlorotNormal(seed = RANDOM_SEED)
                   ))
    model.add(Dropout(0.5))
    model.add(Dense(text_vector_size // 4,
                    activation='relu',
                    kernel_initializer = tf.keras.initializers.GlorotNormal(seed = RANDOM_SEED)
                   ))
    model.add(Dense(image_vector_size // 2,
                    activation='relu',
                    kernel_initializer = tf.keras.initializers.GlorotNormal(seed = RANDOM_SEED)
                   ))
    model.add(Dropout(0.5))
    model.add(Dense(image_vector_size,
                    activation='relu',
                    kernel_initializer = tf.keras.initializers.GlorotNormal(seed = RANDOM_SEED)
                   ))
    model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])

    return model

# Commented out IPython magic to ensure Python compatibility.
# %%time
# nn_model = create_nn_model()
# nn_model.fit(
#     X, Y,
#     #batch_size = 100,
#     epochs = 100,
#     validation_data = (X_val, Y_val),
#     verbose = 1)

"""MSE чуть лучше, чем у линейных моделей.  
Проверим работу этой модели и сравним с другими:
"""

query_text = 'A black dog jumping off a dock'

images = search_image(query_text, y_valid, 4, predict_LR, lr)
show_images(query_text, images, suptitle = 'LinearRegression search for: ')

images = search_image(query_text, y_valid, 4, predict_LR, ridge)
show_images(query_text, images, suptitle = 'Ridge search for: ')

images = search_image(query_text, y_valid, 4, nn_model.predict)
show_images(query_text, images, suptitle = 'NeuroNet search for: ')

"""Нейронная сеть нашла больше собак, темных и в прыжке. Остальное модели не нашли

"""

query_text = 'A person is standing on skis in the snow with a dog'

images = search_image(query_text, y_valid, 4, predict_LR, lr)
show_images(query_text, images, suptitle = 'LinearRegression search for: ')

images = search_image(query_text, y_valid, 4, predict_LR, ridge)
show_images(query_text, images, suptitle = 'Ridge search for: ')

images = search_image(query_text, y_valid, 4, nn_model.predict)
show_images(query_text, images, suptitle = 'NeuroNet search for: ')

"""Людей нашли все, снег и лыжи не нашла линейная регрессия."""

query_text = 'A man in a blue t-shirt holds a camera'

images = search_image(query_text, y_valid, 4, predict_LR, lr)
show_images(query_text, images, suptitle = 'LinearRegression search for: ')

images = search_image(query_text, y_valid, 4, predict_LR, ridge)
show_images(query_text, images, suptitle = 'Ridge search for: ')

images = search_image(query_text, y_valid, 4, nn_model.predict)
show_images(query_text, images, suptitle = 'NeuroNet search for: ')

"""Ridge и нейронная сеть нашли одинаковые изображения, но присвоили им разную вероятность соответствия"""

query_text = 'Man falling off a blue surfboard in the ocean'

images = search_image(query_text, y_valid, 4, predict_LR, lr)
show_images(query_text, images, suptitle = 'LinearRegression search for: ')

images = search_image(query_text, y_valid, 4, predict_LR, ridge)
show_images(query_text, images, suptitle = 'Ridge search for: ')

images = search_image(query_text, y_valid, 4, nn_model.predict)
show_images(query_text, images, suptitle = 'NeuroNet search for: ')

"""Нейронная сеть везде нашла океан и первым и вторым результатом выдала мужчину на серфе. Линейная регрессия путает снег и океан.

### Выводы

В данном разделе были обучены две модели машинного обучения и нейронная сеть сопоставлять текстовое описание и изображение, которое ему подходит;
Для контроля обучения и сравнения результатов была использована метрика MSE: лучший результат показала нейронная сеть;
Было проведено сравнение результатов всех трех моделей - по совокупности метрики и визуальной оценки нейронная сеть работает лучше

### Тестирование моделей

Проведем тестирование моделей. Для этого получим эмбеддинги для всех тестовых изображений из папки test_images, выберем 10 запросов из файла test_queries и для каждого запроса выведем несколько релевантных изображений. Сравним визуально качество поиска:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# test_images['image_vector'] = test_images.apply(lambda x: tf.cast(model(image_to_array(x['image'], TEST_IMG_DIR)), dtype=np.float64),
#                                                 axis = 1)

query_text = 'A black and grey dog catches a tennis ball at the beach.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A woman wears a colorful shirt and a lot of jewelry.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A couple each wearing a silly mask and holding a fake cigar.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 8, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'Red boat marked GRAHAM races across water.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A man demonstrating how high his black dog can jump.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A man is operating a television camera high up in a crowded stadium.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A large gray bird begins to land in the water.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A group of people are in the woods playing instruments.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'A dog is catching a yellow tennis ball on the beach.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

query_text = 'The brown dog is wearing a black collar.'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 4, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

"""Запрос, который нарушает юридические ограничения"""

query_text = 'A playground with two children and an adult'
print(f'Looking for pictures under the description: {query_text}')

images = search_image(query_text, test_images, 8, nn_model.predict)
if images:
    show_images(query_text, images, path = TEST_IMG_DIR, suptitle = 'NeuroNet search for: ')

"""## Итоговый вывод

Все модели показали себя достаточно хорошо.Для финального варианта была выбрана нейронная сеть.В целом, концепция показала свою жизнеспособность и может быть доработана с помощью увеличения датасета и построения более глубокой нейросети с более тщательным подбором гирперпараметров, возможно рассмотреть ансамбль моделей (нейронная сеть + Ridge), чтобы объединить их лучшие стороны.
В качестве исходных данных были использованы: 1000 фотографий для обучения модели, 100 фотографий для тестирования и демонстрации возможностей модели, данные по соответствию изображения и описания, полученные с помощью краудсорсинга и в результате опроса экспертов.
Чтобы научить модель выполнять такие вычисления (преобразования), были отобраны пары описание-изображение, в которых изображение полностью или частично подходит описанию, опираясь на комплексную оценку - если оценка выше 0.5, значит описание больше подходит, чем нет.

Результатом проекта является основная функция search_image, которая объединяет в себе весь процесс поиска и верификации изображений: получает векторное представление текста, векторные представления изображений из базы, рассчитывает для каждой пары метрику на основе косинусного расстояния от 0 до 1, которая показывает, насколько текст и картинка подходят друг другу и отбирает lim лучших изображений, подходящих по строке запроса. Для ее работы необходима предобученная модель, на роль которой в рамках проекта были рассмотрены три варианты: LinearRegression, Ridge и нейронная сеть.

"""









